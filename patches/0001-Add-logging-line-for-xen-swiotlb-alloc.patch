From 47755c0845427d65db3073e6956c52d5eccc2a25 Mon Sep 17 00:00:00 2001
From: Benjamin Leggett <benjamin@edera.io>
Date: Tue, 17 Jun 2025 17:06:53 -0400
Subject: [PATCH] Add logging line for xen swiotlb alloc

---
 arch/x86/xen/mmu_pv.c                         | 20 +++++++++++++++++--
 .../gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c  |  5 +++++
 drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c       |  7 +++++++
 drivers/iommu/dma-iommu.c                     |  4 +++-
 drivers/xen/swiotlb-xen.c                     | 17 ++++++++++++----
 include/linux/swiotlb.h                       |  4 ++--
 kernel/dma/mapping.c                          | 11 +++++++---
 kernel/dma/swiotlb.c                          | 14 ++++++++++---
 8 files changed, 67 insertions(+), 15 deletions(-)

diff --git a/arch/x86/xen/mmu_pv.c b/arch/x86/xen/mmu_pv.c
index d078de2c952b..f765b9eff185 100644
--- a/arch/x86/xen/mmu_pv.c
+++ b/arch/x86/xen/mmu_pv.c
@@ -313,15 +313,21 @@ static bool xen_batched_set_pte(pte_t *ptep, pte_t pteval)
 {
 	struct mmu_update u;
 
-	if (xen_get_lazy_mode() != XEN_LAZY_MMU)
+	if (xen_get_lazy_mode() != XEN_LAZY_MMU) {
+		printk(KERN_INFO "BML: not using LAZY_MMU\n");
 		return false;
+	}
 
 	xen_mc_batch();
 
+	printk(KERN_INFO "BML: starting call batch\n");
 	u.ptr = virt_to_machine(ptep).maddr | MMU_NORMAL_PT_UPDATE;
+	printk(KERN_INFO "BML: pte val\n");
 	u.val = pte_val_ma(pteval);
+	printk(KERN_INFO "BML: mmu update with ptr addr: %llx and val: %llx \n", u.ptr, u.val);
 	xen_extend_mmu_update(&u);
 
+	printk(KERN_INFO "BML: issue batch\n");
 	xen_mc_issue(XEN_LAZY_MMU);
 
 	return true;
@@ -329,7 +335,10 @@ static bool xen_batched_set_pte(pte_t *ptep, pte_t pteval)
 
 static inline void __xen_set_pte(pte_t *ptep, pte_t pteval)
 {
+
+	printk(KERN_INFO "BML: __xen_set_pte: %p, val: %lx\n", ptep, pteval.pte);
 	if (!xen_batched_set_pte(ptep, pteval)) {
+		printk(KERN_INFO "BML: prepping hypercall\n");
 		/*
 		 * Could call native_set_pte() here and trap and
 		 * emulate the PTE write, but a hypercall is much cheaper.
@@ -337,7 +346,9 @@ static inline void __xen_set_pte(pte_t *ptep, pte_t pteval)
 		struct mmu_update u;
 
 		u.ptr = virt_to_machine(ptep).maddr | MMU_NORMAL_PT_UPDATE;
+		printk(KERN_INFO "BML: pte val\n");
 		u.val = pte_val_ma(pteval);
+		printk(KERN_INFO "BML: making MMU update hypercall with ptr addr: %llx and val: %llx \n", u.ptr, u.val);
 		HYPERVISOR_mmu_update(&u, 1, NULL, DOMID_SELF);
 	}
 }
@@ -398,6 +409,8 @@ static pteval_t pte_pfn_to_mfn(pteval_t val)
 
 		mfn = __pfn_to_mfn(pfn);
 
+		printk(KERN_INFO "BML: pte_pfn_to_mfn: Got MFN: %lx\n", mfn);
+
 		/*
 		 * If there's no mfn for the pfn, then just create an
 		 * empty non-present pte.  Unfortunately this loses
@@ -405,6 +418,7 @@ static pteval_t pte_pfn_to_mfn(pteval_t val)
 		 * pte_mfn_to_pfn is asymmetric.
 		 */
 		if (unlikely(mfn == INVALID_P2M_ENTRY)) {
+			printk(KERN_INFO "BML: pte_pfn_to_mfn: invalid P2M entry: %lx\n", val);
 			mfn = 0;
 			flags = 0;
 		} else
@@ -2414,8 +2428,10 @@ void xen_destroy_contiguous_region(phys_addr_t pstart, unsigned int order)
 	int success;
 	unsigned long vstart;
 
-	if (unlikely(order > discontig_frames_order))
+	if (unlikely(order > discontig_frames_order)) {
+		printk(KERN_INFO "order > discontig_order, skipping destroy\n");
 		return;
+	}
 
 	vstart = (unsigned long)phys_to_virt(pstart);
 	memset((void *) vstart, 0, PAGE_SIZE << order);
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c
index 1e998f972c30..96bc48dea01a 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c
@@ -20,6 +20,7 @@
  * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
  * OTHER DEALINGS IN THE SOFTWARE.
  */
+#include <linux/types.h>
 #include <linux/dma-buf.h>
 #include <linux/list.h>
 #include <linux/pagemap.h>
@@ -569,6 +570,10 @@ kfd_mem_dmamap_userptr(struct kgd_mem *mem,
 		return -ENOMEM;
 
 	/* Same sequence as in amdgpu_ttm_tt_pin_userptr */
+	/* size_t max_segment = 0; */
+	/* max_segment = dma_max_mapping_size(adev->dev); */
+	/* if (max_segment == 0) */
+	/* 	max_segment = UINT_MAX; */
 	ret = sg_alloc_table_from_pages(ttm->sg, src_ttm->pages,
 					ttm->num_pages, 0,
 					(u64)ttm->num_pages << PAGE_SHIFT,
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c
index 262bd010a283..0adf13100b9e 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c
@@ -806,6 +806,11 @@ static int amdgpu_ttm_tt_pin_userptr(struct ttm_device *bdev,
 		DMA_BIDIRECTIONAL : DMA_TO_DEVICE;
 	int r;
 
+
+	/* size_t max_segment = 0; */
+	/* max_segment = dma_max_mapping_size(adev->dev); */
+	/* if (max_segment == 0) */
+	/* 	max_segment = UINT_MAX; */
 	/* Allocate an SG array and squash pages into it */
 	r = sg_alloc_table_from_pages(ttm->sg, ttm->pages, ttm->num_pages, 0,
 				      (u64)ttm->num_pages << PAGE_SHIFT,
@@ -825,8 +830,10 @@ static int amdgpu_ttm_tt_pin_userptr(struct ttm_device *bdev,
 	return 0;
 
 release_sg_table:
+	printk(KERN_INFO "BML: couldn't DMA map sg table\n");
 	sg_free_table(ttm->sg);
 release_sg:
+	printk(KERN_INFO "BML: couldn't alloc sg table from pages\n");
 	kfree(ttm->sg);
 	ttm->sg = NULL;
 	return r;
diff --git a/drivers/iommu/dma-iommu.c b/drivers/iommu/dma-iommu.c
index 2a9fa0c8cc00..c1b75833c61c 100644
--- a/drivers/iommu/dma-iommu.c
+++ b/drivers/iommu/dma-iommu.c
@@ -1738,8 +1738,10 @@ size_t iommu_dma_opt_mapping_size(void)
 
 size_t iommu_dma_max_mapping_size(struct device *dev)
 {
-	if (dev_is_untrusted(dev))
+	if (dev_is_untrusted(dev)) {
+		printk(KERN_INFO "BML: untrusted dev, returning SWIOTLB max size\n");
 		return swiotlb_max_mapping_size(dev);
+	}
 
 	return SIZE_MAX;
 }
diff --git a/drivers/xen/swiotlb-xen.c b/drivers/xen/swiotlb-xen.c
index 1f65795cf5d7..a79ed62b6239 100644
--- a/drivers/xen/swiotlb-xen.c
+++ b/drivers/xen/swiotlb-xen.c
@@ -109,6 +109,7 @@ static struct io_tlb_pool *xen_swiotlb_find_pool(struct device *dev,
 	 */
 	if (pfn_valid(PFN_DOWN(paddr)))
 		return swiotlb_find_pool(dev, paddr);
+	printk(KERN_INFO "SEEMS: didn't find address in domain's pool: addr %llx, DMA addr %llx, Xen PFN: %lx, BFN: %lx \n", paddr, dma_addr, xen_pfn, bfn);
 	return NULL;
 }
 
@@ -131,8 +132,10 @@ int __init xen_swiotlb_fixup(void *buf, unsigned long nslabs)
 				p + (i << IO_TLB_SHIFT), order,
 				dma_bits, &dma_handle);
 		} while (rc && dma_bits++ < MAX_DMA_BITS);
-		if (rc)
+		if (rc) {
+			printk(KERN_INFO "SEEMS bombed in slab alloc\n");
 			return rc;
+		}
 
 		i += IO_TLB_SEGSIZE;
 	} while (i < nslabs);
@@ -152,8 +155,10 @@ xen_swiotlb_alloc_coherent(struct device *dev, size_t size,
 	size = ALIGN(size, XEN_PAGE_SIZE);
 
 	ret = (void *)__get_free_pages(flags, get_order(size));
-	if (!ret)
+	if (!ret) {
+		printk(KERN_WARNING "SEEMS bombed on get_free_pages\n");
 		return ret;
+	}
 	phys = virt_to_phys(ret);
 
 	*dma_handle = xen_phys_to_dma(dev, phys);
@@ -170,6 +175,7 @@ xen_swiotlb_alloc_coherent(struct device *dev, size_t size,
 	return ret;
 
 out_free_pages:
+	printk(KERN_WARNING "SEEMS like bombed on create_contiguous_region\n");
 	free_pages((unsigned long)ret, get_order(size));
 	return NULL;
 }
@@ -246,10 +252,13 @@ static dma_addr_t xen_swiotlb_map_page(struct device *dev, struct page *page,
 
 done:
 	if (!dev_is_dma_coherent(dev) && !(attrs & DMA_ATTR_SKIP_CPU_SYNC)) {
-		if (pfn_valid(PFN_DOWN(dma_to_phys(dev, dev_addr))))
+		if (pfn_valid(PFN_DOWN(dma_to_phys(dev, dev_addr)))) {
+			printk(KERN_INFO "SEEMS like a valid PFN, using arch dma sync %llx\n", dev_addr);
 			arch_sync_dma_for_device(phys, size, dir);
-		else
+		} else {
+			printk(KERN_INFO "SEEMS like an invalid PFN, using xen dma sync %llx\n", dev_addr);
 			xen_dma_sync_for_device(dev, dev_addr, size, dir);
+		}
 	}
 	return dev_addr;
 }
diff --git a/include/linux/swiotlb.h b/include/linux/swiotlb.h
index 3dae0f592063..342daa7d84d2 100644
--- a/include/linux/swiotlb.h
+++ b/include/linux/swiotlb.h
@@ -23,13 +23,13 @@ struct scatterlist;
  * must be a power of 2.  What is the appropriate value ?
  * The complexity of {map,unmap}_single is linearly dependent on this value.
  */
-#define IO_TLB_SEGSIZE	128
+#define IO_TLB_SEGSIZE 32768
 
 /*
  * log of the size of each IO TLB slab.  The number of slabs is command line
  * controllable.
  */
-#define IO_TLB_SHIFT 11
+#define IO_TLB_SHIFT 12
 #define IO_TLB_SIZE (1 << IO_TLB_SHIFT)
 
 /* default to 64MB */
diff --git a/kernel/dma/mapping.c b/kernel/dma/mapping.c
index cda127027e48..2311603a09d2 100644
--- a/kernel/dma/mapping.c
+++ b/kernel/dma/mapping.c
@@ -223,6 +223,7 @@ static int __dma_map_sg_attrs(struct device *dev, struct scatterlist *sg,
 		debug_dma_map_sg(dev, sg, nents, ents, dir, attrs);
 	} else if (WARN_ON_ONCE(ents != -EINVAL && ents != -ENOMEM &&
 				ents != -EIO && ents != -EREMOTEIO)) {
+		// TODO(bml) this
 		trace_dma_map_sg_err(dev, sg, nents, ents, dir, attrs);
 		return -EIO;
 	}
@@ -937,12 +938,16 @@ size_t dma_max_mapping_size(struct device *dev)
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 	size_t size = SIZE_MAX;
 
-	if (dma_map_direct(dev, ops))
+	if (dma_map_direct(dev, ops)) {
 		size = dma_direct_max_mapping_size(dev);
-	else if (use_dma_iommu(dev))
+		printk(KERN_INFO "BML: DMA direct max mapping size %zu\n", size);
+	} else if (use_dma_iommu(dev)) {
 		size = iommu_dma_max_mapping_size(dev);
-	else if (ops && ops->max_mapping_size)
+		printk(KERN_INFO "BML: DMA iommu max mapping size %zu\n", size);
+	} else if (ops && ops->max_mapping_size) {
 		size = ops->max_mapping_size(dev);
+		printk(KERN_INFO "BML: DMA max mapping size %zu\n", size);
+	}
 
 	return size;
 }
diff --git a/kernel/dma/swiotlb.c b/kernel/dma/swiotlb.c
index abcf3fa63a56..de75b1d83d8f 100644
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@ -1090,6 +1090,7 @@ static int swiotlb_search_pool_area(struct device *dev, struct io_tlb_pool *pool
 	}
 
 not_found:
+	printk(KERN_INFO "BML: out of slots! nslots: %u, pool slabs: %u, used area: %lu\n", nslots, pool->area_nslabs, area->used);
 	spin_unlock_irqrestore(&area->lock, flags);
 	return -1;
 
@@ -1190,8 +1191,10 @@ static int swiotlb_find_slots(struct device *dev, phys_addr_t orig_addr,
 	int cpu, i;
 	int index;
 
-	if (alloc_size > IO_TLB_SEGSIZE * IO_TLB_SIZE)
+	if (alloc_size > IO_TLB_SEGSIZE * IO_TLB_SIZE) {
+		printk(KERN_INFO "BML: alloc_size %zu greater than SEGSIZE %d times TLBSIZE %d\n", alloc_size, IO_TLB_SEGSIZE, IO_TLB_SIZE);
 		return -1;
+	}
 
 	cpu = raw_smp_processor_id();
 	for (i = 0; i < default_nareas; ++i) {
@@ -1201,8 +1204,10 @@ static int swiotlb_find_slots(struct device *dev, phys_addr_t orig_addr,
 			goto found;
 	}
 
-	if (!mem->can_grow)
+	if (!mem->can_grow) {
+		printk(KERN_INFO "BML: memory ungrowable\n");
 		return -1;
+	}
 
 	schedule_work(&mem->dyn_alloc);
 
@@ -1210,13 +1215,16 @@ static int swiotlb_find_slots(struct device *dev, phys_addr_t orig_addr,
 	phys_limit = min_not_zero(*dev->dma_mask, dev->bus_dma_limit);
 	pool = swiotlb_alloc_pool(dev, nslabs, nslabs, 1, phys_limit,
 				  GFP_NOWAIT | __GFP_NOWARN);
-	if (!pool)
+	if (!pool) {
+		printk(KERN_INFO "BML: can't alloc pool\n");
 		return -1;
+	}
 
 	index = swiotlb_search_pool_area(dev, pool, 0, orig_addr,
 					 alloc_size, alloc_align_mask);
 	if (index < 0) {
 		swiotlb_dyn_free(&pool->rcu);
+		printk(KERN_INFO "BML: pool search failed\n");
 		return -1;
 	}
 
-- 
2.49.0

